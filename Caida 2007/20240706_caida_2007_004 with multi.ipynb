{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPw8eCx62mka"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGdSVPBM5gJg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur6gJVo15gL7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5IYYZTO5gOQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5cxfMij2rxk",
        "outputId": "00ebce24-b725-4b1e-e2eb-791050e660b8"
      },
      "outputs": [],
      "source": [
        "print(\"For preprocessing the caida 2007 Dataset!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU1Du3y2HfER"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_entropy(data_list):\n",
        "  count = Counter(data_list)\n",
        "  probabilities = [count[key] / len(data_list) for key in count.keys()]\n",
        "  entropy = -sum([p * math.log2(p) for p in probabilities])\n",
        "  return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_and_print_statistics(dataset):\n",
        "  try:\n",
        "      dataset = dataset\n",
        "\n",
        "      src_ip_entropy = calculate_entropy(dataset['Source_IP'])\n",
        "      src_port_entropy = calculate_entropy(dataset['Source_Port'])\n",
        "      dst_port_entropy = calculate_entropy(dataset['Destination_Port'])\n",
        "      protocol_entropy = calculate_entropy(dataset['Protocol'])\n",
        "      total_packets = len(dataset['No.'])\n",
        "      bandwidth = dataset['Length'].sum()\n",
        "    #   print(f\"Entropy of source IP address (etpSrcIP): {src_ip_entropy}\")\n",
        "    #   print(f\"Entropy of source port (etpSrcP): {src_port_entropy}\")\n",
        "    #   print(f\"Entropy of destination port (etpDstP): {dst_port_entropy}\")\n",
        "    #   print(f\"Entropy of packet protocol (etpProtocol): {protocol_entropy}\")\n",
        "    #   print(f\"Total number of packets (totalPacket): {total_packets}\")\n",
        "    #   print(f\"Total bandwidth (totalBandwidth): {bandwidth}\")\n",
        "      return (src_ip_entropy, src_port_entropy, dst_port_entropy, protocol_entropy, total_packets,bandwidth)\n",
        "  except Exception as e:\n",
        "      print(f\"Error calculating statistics: {e}\")\n",
        "      \n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_data_in_two_second_intervals_with_entropy(df, main_df):\n",
        "\n",
        "\n",
        "    # Define the time interval (every 2 seconds)\n",
        "    time_interval = pd.Timedelta(seconds=2)\n",
        "\n",
        "    # Initialize start time as the first timestamp in the DataFrame\n",
        "    start_time = df['Time'].iloc[0]\n",
        "\n",
        "    # List to store DataFrames for each interval\n",
        "    iw = 0\n",
        "    total = 0\n",
        "    min = 99999999999999999\n",
        "    while start_time <= df['Time'].iloc[-1]:\n",
        "        # Define end time as start time plus time interval\n",
        "        end_time = start_time + time_interval\n",
        "        \n",
        "        \n",
        "        # Filter data for the current time interval\n",
        "        interval_data = df[(df['Time'] >= start_time) & (df['Time'] < end_time)]\n",
        "        vals = calculate_and_print_statistics(interval_data)\n",
        "        # main_df = main_df + vals\n",
        "        main_df.loc[len(main_df)] = vals\n",
        "        # Append interval data to the list of DataFrames\n",
        "        # interval_dfs.append(interval_data)\n",
        "        length = len(interval_data)\n",
        "        total = total + length\n",
        "        if length < min:\n",
        "            min = length\n",
        "        \n",
        "        # Update start time for the next interval\n",
        "        start_time = end_time\n",
        "        print(f\"\\rInterval {iw+1} completed...\")\n",
        "        iw = iw+1\n",
        "        print(\"--------------------------------------\")\n",
        "    print(\"len of total \", total) \n",
        "    print(\"Min per 2s \", min)\n",
        "    return main_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwHOTzK3HfKq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def main(parquet_file, label):\n",
        "    # Read Parquet file into a PyArrow Table\n",
        "    table = pq.read_table(parquet_file)\n",
        "\n",
        "    # Convert PyArrow Table to pandas DataFrame\n",
        "    df = table.to_pandas()\n",
        "    # Convert timestamp from scientific notation to regular floating-point numbers\n",
        "    df['Time'] = df['Time'].astype(float)\n",
        "\n",
        "    # Convert 'Time' column to datetime format\n",
        "    df['Time'] = pd.to_datetime(df['Time'], unit='s')\n",
        "\n",
        "    # Sort DataFrame by 'Time' column to ensure it's in chronological order\n",
        "    df = df.sort_values(by='Time')\n",
        "\n",
        "    main_dfv = ['etpSrcIP','etpSrcP','etpDstP','etpProtocol','totalPacket','bandwidth']\n",
        "    main_df = pd.DataFrame(columns = main_dfv)\n",
        "    main_df_filled = pd.DataFrame(columns = main_dfv)\n",
        "\n",
        "    # Making and saving\n",
        "    main_df = save_data_in_two_second_intervals_with_entropy(df, main_df)\n",
        "    main_df['label'] = label\n",
        "    file_name = parquet_file.replace('.pcap output_dataframe.parquet', '2s unlabeled_unfilled_entropy.csv')\n",
        "    main_df.to_csv('done/unfilled/' + file_name, index=False)\n",
        "    \n",
        "    # Replace NaN values with 0\n",
        "    df_filled = df.fillna(0)    \n",
        "    main_df_filled = save_data_in_two_second_intervals_with_entropy(df_filled, main_df_filled)\n",
        "    main_df_filled['label'] = label\n",
        "    file_name = parquet_file.replace('.pcap output_dataframe.parquet', '2s unlabeled_filled_entropy.csv')\n",
        "    main_df_filled.to_csv('done/filled/' + file_name, index=False)\n",
        "    \n",
        "    print(\"len of df \", len(df))\n",
        "    print(\"len of df filled \",len(df_filled))\n",
        "    print(\"len of main df\", len(main_df))\n",
        "    print(\"len of main df filled\", len(main_df_filled)) \n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "parquet_files_0 = []\n",
        "parquet_files_1 = []\n",
        "\n",
        "#labeling to 0\n",
        "parquet_files_0.append('ddostrace.to-victim.20070804_134936.pcap output_dataframe.parquet')\n",
        "parquet_files_0.append('ddostrace.to-victim.20070804_135436.pcap output_dataframe.parquet')\n",
        "parquet_files_0.append('ddostrace.to-victim.20070804_135936.pcap output_dataframe.parquet')\n",
        "parquet_files_0.append('ddostrace.to-victim.20070804_140436.pcap output_dataframe.parquet')\n",
        "\n",
        "# ## Doubtfull\n",
        "\n",
        "# parquet_file = 'ddostrace.to-victim.20070804_140936.pcap output_dataframe.parquet'\n",
        "\n",
        "# #labeling to 1\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_141436.pcap output_dataframe.parquet')\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_141936.pcap output_dataframe.parquet')\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_142436.pcap output_dataframe.parquet')\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_142936.pcap output_dataframe.parquet')\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_143436.pcap output_dataframe.parquet')\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_143936.pcap output_dataframe.parquet')\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_144436.pcap output_dataframe.parquet')\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_144936.pcap output_dataframe.parquet')\n",
        "parquet_files_1.append('ddostrace.to-victim.20070804_145436.pcap output_dataframe.parquet')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for parquet_file0 in parquet_files_0:\n",
        "    main(parquet_file0, 0)\n",
        "print(\"With 0 completed!!!\")\n",
        "for parquet_file1 in parquet_files_1:\n",
        "    main(parquet_file1, 1)\n",
        "print(\"With 1 completed!!!\")\n",
        "print(\"Done!!!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
